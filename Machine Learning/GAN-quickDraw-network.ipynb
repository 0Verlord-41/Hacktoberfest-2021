{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN using quick draw dataset of apple in npy format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select processing devices if you have a gpu(for nvidia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data input and output:\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for deep learning: \n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import Activation, Reshape, Conv2DTranspose, UpSampling2D # new! \n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# for plotting: \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "## Batch normalization is used to normalize the output since we have to many\n",
    "## layers to we normalize the mean, std etc of our output\n",
    "## Dropout is used to drop on layers or nodes in a layer to prevent overfitting\n",
    "## Conv2DTranspose is opposite of Conv2D layer, it is deconvolution layer\n",
    "## UpSampling2D is opposite of MaxPooling2D, In max pooling we decrease size\n",
    "## of feature maps(or image), here we upscale them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = \"apple.npy\"\n",
    "data = np.load(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/255\n",
    "data = data.reshape(data.shape[0],28,28,1)\n",
    "## resshaping image from (rows,784) to (rows,28,28,1) where 1 is channel for \n",
    "## grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = data.shape[1:3] ## storing height and width i.e 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[3,:,:,0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Discriminator network\n",
    "#### NOTE: width can be called as depth of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where depth is the number of neurons and p is the dropout rate\n",
    "# the dropout rate is 40% here on given training update\n",
    "# these are hyper paramters that can be changed according to need\n",
    "def discriminator_builder(depth=64,p=0.4):\n",
    "    ## define inputs\n",
    "    inputs = Input((img_w,img_h,1))\n",
    "    ## 28x28x1\n",
    "    ## Convolution layers:\n",
    "        ## here depth maybe the number of feature maps where we \n",
    "        ## multiplied by 1 just in case we want to increase num of maps \n",
    "        ## we change the 1 to twice or depth paramter\n",
    "        ## also \n",
    "    conv1 = Conv2D(depth*1, (5,5), strides=2, padding=\"same\", activation='relu'\n",
    "                   #input_shape=inputs\n",
    "                   #we can use two ways of specifying input to this layer\n",
    "                  )(inputs)\n",
    "    ## adding a dropout too\n",
    "    ## i think drop out drops some of the feature maps maybe?\n",
    "    ## here output of this layer is 64 feature maps so we drop 40% maybe\n",
    "    ## and give 60% to the next layer, not sure and need to confirm\n",
    "    conv1 = Dropout(p)(conv1)\n",
    "    \n",
    "    ## having 128 maps now since depth=64 and * 2 = 128\n",
    "    conv2 = Conv2D(depth*2, (5,5), strides=2, padding=\"same\", \n",
    "                   activation='relu')(conv1)\n",
    "    conv2 = Dropout(p)(conv2)\n",
    "        \n",
    "        \n",
    "    conv3 = Conv2D(depth*4, (5,5), strides=2, padding=\"same\", \n",
    "                   activation='relu')(conv2)\n",
    "    conv3 = Dropout(p)(conv3)\n",
    "    ## note we are manually connecting layers as you can see we give\n",
    "    ## conv1 as input to conv2 layer and so on\n",
    "    \n",
    "    conv4 = Conv2D(depth*8, (5,5), strides=1, padding=\"same\", \n",
    "                   activation='relu')(conv3)\n",
    "    conv4 = Dropout(p)(conv4)\n",
    "    \n",
    "    ## time to flatten yo\n",
    "    conv4 = Flatten()(conv4)\n",
    "    \n",
    "    ## output layer\n",
    "    ## the output is one since it's binary whether the image is fake or real\n",
    "    ## in this case whether the image has apple or not(true or false)\n",
    "    output = Dense(1, activation=\"sigmoid\")(conv4)\n",
    "    \n",
    "    ## Model definition\n",
    "    ## input that we defined in beginning of function and output as 2 lines above\n",
    "    model = Model(inputs=inputs,outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decay is how learning rate slows down\n",
    "## clipvalue kinda makes sure the parameter don't jump around too much\n",
    "discriminator.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=RMSprop(lr=0.0008,decay=6e-8, clipvalue=1.0),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_builder(z_dim=100,depth=64,p=0.4):\n",
    "    \n",
    "    # Define inputs\n",
    "    inputs = Input((z_dim,))\n",
    "    \n",
    "    ## first dense layer\n",
    "    ## here 7*7 can be seen as a shape like 28x28 so we construct an image\n",
    "    ## from dense layer since it's opposite of discriminitor that converts\n",
    "    ## an image to dense. Writing this for understanding\n",
    "    dense1 = Dense(7*7*64)(inputs)\n",
    "    \n",
    "    ## we use batch normalization to maintain mean, variation of paramters\n",
    "    ## through out the layers of our network, we can't risk variations as \n",
    "    ## training is being done\n",
    "    ## momentum is like flexibility, we allow how much far variation can happen\n",
    "    dense1 = BatchNormalization(momentum=0.9)(dense1) # default momentum for moving average is 0.99\n",
    "    \n",
    "    ## we are using activation after normalization rather before it\n",
    "    dense1 = Activation(activation='relu')(dense1)\n",
    "    \n",
    "    ## we reshape our dense layer into to 3D 7x7x64 width,height,depth\n",
    "    ## we reshape it so that we can feed it to discriminator\n",
    "    dense1 = Reshape((7,7,64))(dense1)\n",
    "    dense1 = Dropout(p)(dense1)\n",
    "    \n",
    "    ## now comes our deconvolutional part\n",
    "    ## going to take 7x7 input and will up-sample to larger\n",
    "    conv1 = UpSampling2D()(dense1)\n",
    "    \n",
    "    ## deconvolution layer reduces depth exponentially as we approach\n",
    "    ## in network to discriminator.\n",
    "    ## kernel_size should be same as above discrminitor kernel size \n",
    "    ## which is (5,5)\n",
    "    ## activation is none since we will do it after batch normalization\n",
    "    conv1 = Conv2DTranspose(int(depth/2), kernel_size=5, padding='same', activation=None,)(conv1)\n",
    "    conv1 = BatchNormalization(momentum=0.9)(conv1)\n",
    "    conv1 = Activation(activation='relu')(conv1)\n",
    "    \n",
    "    ## second deconv layer\n",
    "    ## Conv2 takes Conv1 as input\n",
    "    conv2 = UpSampling2D()(conv1)\n",
    "    conv2 = Conv2DTranspose(int(depth/4), kernel_size=5, padding='same', activation=None,)(conv2)\n",
    "    conv2 = BatchNormalization(momentum=0.9)(conv2)\n",
    "    conv2 = Activation(activation='relu')(conv2)\n",
    "    \n",
    "    ## third deconv layer, we can have as many depending on situation\n",
    "    ## Conv1 takes Conv2 as input\n",
    "    ## not up-sampling here else size will be too large\n",
    "    conv3 = Conv2DTranspose(int(depth/8), kernel_size=5, padding='same', activation=None,)(conv2)\n",
    "    conv3 = BatchNormalization(momentum=0.9)(conv3)\n",
    "    conv3 = Activation(activation='relu')(conv3)\n",
    "\n",
    "    ## output layer:\n",
    "    ## it is a convolutional layer which has single feature map output.\n",
    "    ## we are using sigmoid since we want the feature map to have values between\n",
    "    ## 0 and 1 where 0 would represent white and 1 is black\n",
    "    ## since our discriminator takes values between 0 and 1 so we create matrix\n",
    "    ## or img or feature map values between 0 and 1\n",
    "    output = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(conv3)\n",
    "    \n",
    "    ## model definition\n",
    "    ## same way as discriminator\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### As you can see how we converted dense layer from 3136 to 7x7x64, then upsampling the size of 7x7 to 14x14 while reducing depth to 32 and then upsampling size of 14x14 to 28x28 while reducing depth to 16,8,1 now 28x28x1 is basically our input to discriminator.\n",
    " ---\n",
    " ## NOTE:\n",
    " #### we kept momentum of layers and size between discriminator and generator and successfully created 28x28x1 image/matrix from dense layers. #### It will be noise generation by generator at first but will update with time to improve the image it generates and get accurate as possible with time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create adversarial Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## z_dim is latent space\n",
    "def adversarial_builder(z_dim=100):\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=RMSprop(lr=0.004, decay=3e-8, clipvalue=1.0),\n",
    "                      metrics=['accuracy']\n",
    "                     )\n",
    "        model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_model = adversarial_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes neural net as input and a boolean which the variable val\n",
    "## the val represent whether the network should be trainable or not on \n",
    "## on our command\n",
    "## The discriminator is used twice. One on it's own to detect fake and\n",
    "## fake images. During that we want to be training the discriminator net\n",
    "## second, we freeze the weights of the discriminator with this function\n",
    "## so that we aren't changing weights of discriminator, instead we change the\n",
    "## the weights of generator.\n",
    "\n",
    "\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=2000, batch=128):\n",
    "    #we save accuracy as we train the nets\n",
    "    d_metrics= [] # discriminator metrics\n",
    "    a_metrics= [] # adversarial metrics\n",
    "    \n",
    "    running_d_lose = 0 # running loss discriminator\n",
    "    running_d_acc = 0 # running acc discriminator\n",
    "    \n",
    "    running_a_lose = 0 # running loss advarsarial\n",
    "    running_a_acc = 0 # running acc advarsarial\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ## modular operation here\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "            \n",
    "        real_imgs = np.reshape(data[np.random.choice(data.shape[0],batch,replace=False)],\n",
    "                               (batch,28,28,1)\n",
    "                              )\n",
    "        ## images generated by generator, we give intial noise to generator\n",
    "        ## before training our generator\n",
    "        ## has 100 dimension latent space i.e the size\n",
    "        ## should have same number of real and fake images so w provide batch\n",
    "        fake_imgs = generator.predict(np.random.uniform(-1.0,1.0,size=[batch,100]))\n",
    "        \n",
    "        ## concatenate so we can give input to discriminator,\n",
    "        ## x variable of training, we give fake and real images to \n",
    "        ## discriminator for training\n",
    "        ## y is the label which tells which image are real and which are not\n",
    "        ## so we tell discriminator which images are real and which are fake\n",
    "        ## by using 0 or 1(1 for real)\n",
    "        x= np.concatenate((real_imgs,fake_imgs))\n",
    "        y = np.ones([2*batch,1])\n",
    "        \n",
    "        ## this selects rows starting from 128 to all next and assign them 0\n",
    "        ## as we know x array values from 128 and so on are fakes so we use y\n",
    "        ## those values as fake\n",
    "        y[batch:,:] = 0\n",
    "        \n",
    "        make_trainable(discriminator, True)\n",
    "        \n",
    "        ## train_on_batch does the training of discriminator\n",
    "        d_metrics.append(discriminator.train_on_batch(x,y))\n",
    "        \n",
    "        ## -1 indicate last value and 0 index in of loss which we will get \n",
    "        ## during training\n",
    "        running_d_lose += d_metrics[-1][0] \n",
    "        running_d_acc += d_metrics[-1][1]\n",
    "        \n",
    "        \n",
    "        make_trainable(discriminator, False)\n",
    "        \n",
    "        ## this time we label random noise as real images\n",
    "        ## we use them to update the generator during training. the weights\n",
    "        ## of discriminator are freezed during this time.\n",
    "        ## we calculate the loss between this random noise and discriminator\n",
    "        ## real images to update the generator.\n",
    "        ## Note: this random noise is not used to train discrminator\n",
    "        noise = np.random.uniform(-1.0,1.0,size=[batch,100])\n",
    "        y = np.ones([batch,1])\n",
    "        \n",
    "        \n",
    "        ## okay so we are giving a random batch of 128 size noise to input. that noise input for generator and the generator\n",
    "        ## converts that noise into image/feature-map which is feeded into discriminator which the discriminator\n",
    "        ## and it is shown to discriminator that the image is real. The discriminator determines whether image is fake/real\n",
    "        ## calculates the loss but doesn't update itself since layers are frozen. the loss further is used to calculate\n",
    "        ## the advsersairal loss meaning the whole network loss which is then used to update the network. since discriminator\n",
    "        ## has frozen layers so it wont be updated, instead only the generator will be updated with the optimizer.\n",
    "        ## the whole network loss will help make generator better in making accurate images\n",
    "        a_metrics.append(adversarial_model.train_on_batch(noise,y))\n",
    "        \n",
    "        ## -1 indicate last value and 0 index in of loss which we will get \n",
    "        ## during training\n",
    "        running_a_lose += a_metrics[-1][0] \n",
    "        running_a_acc += a_metrics[-1][1]\n",
    "        \n",
    "        if (i+1)%500 == 0:\n",
    "            ## printing logs every 500 epochs\n",
    "            print('Epoch #{}'.format(i+1))\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, running_d_loss/i, running_d_acc/i)\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, running_a_loss/i, running_a_acc/i)\n",
    "            print(log_mesg)\n",
    "\n",
    "            ## checking output of generator for a random noise\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            plt.figure(figsize=(5,5))\n",
    "\n",
    "            ## plotting generator image to show\n",
    "            for k in range(gen_imgs.shape[0]):\n",
    "                plt.subplot(4, 4, k+1)\n",
    "                plt.imshow(gen_imgs[k, :, :, 0], cmap='gray')\n",
    "                plt.axis('off')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return a_metrics, d_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_metrics_complete, d_metrics_complete = train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ploting the loss and accuracy in dataframe way. a_metrics_complete contains 2D matrix, one part contains loss and other acc\n",
    "## since there are multiple values so we ta\n",
    "ke one row at time and take that rows 0 column which is the loss column in this case\n",
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generator': [metric[0] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[0] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Loss', logy=True)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generator': [metric[1] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[1] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Accuracy')\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: I've done this code on google collab for faster results, It was good one. The comments made are used to clear up confusion or are used to make it easier to understand GAN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
