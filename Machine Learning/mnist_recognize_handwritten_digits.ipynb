{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize handwritten digits with advanced training methods\n",
    "----------------------\n",
    "\n",
    "- Tensorflow-GPU Version: 2.2\n",
    "- GPU trained on: RTX 2080 TI 11GB Vram\n",
    "- Tweak the batchsize if your GPU has not enough vram. \n",
    "\n",
    "----------------------\n",
    "1. Import the important librarys\n",
    "2. Load the data\n",
    "3. Normalize the data\n",
    "4. Show a few samples from the dataset\n",
    "5. One-Hot-Encoding\n",
    "6. Split the data into Trainingdata and Testdata\n",
    "7. Expand the dimensions of the data \n",
    "8. Build the model of the neuronal network\n",
    "9. Choose an Optimizer\n",
    "10. Compile the model\n",
    "11. Configure the Callback \"ReduceLROnPlateau\"\n",
    "12. Configure the Callback \"ModelCheckpoint\"\n",
    "13. Setup the ImageDataGenerator for Data Augementation\n",
    "14. Train the model \n",
    "15. Test the model on unknown data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the important librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten,\n",
    "                          GlobalAveragePooling2D, Input, MaxPool2D)\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical  # convert to one-hot-encoding\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "print(Y_train.shape , X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show a few samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAANqUlEQVR4nO3db6hc9Z3H8c9nTSVo+kDNVUMSN271gSKYlsE/RGqXaFFRVKKlCkVRNoUoVilhgys2DwR12bb0gVSva2yqrlLUEBXdbYwFU4TgqFmNG3bjSjaNxuTGPDB9YGridx/c4+41zvzmZubMH/N9v+AyM+d7zj1fj/eTMzO/M/NzRAjAke+vht0AgMEg7EAShB1IgrADSRB2IIkZg9zZ7NmzY8GCBYPcJZDKtm3btGfPHreq9RR225dI+pWkoyT9c0TcV1p/wYIFajabvewSQEGj0Whb6/ppvO2jJD0g6VJJZ0q6zvaZ3f4+AP3Vy2v2cyS9FxHvR8RfJD0l6cp62gJQt17CPlfSn6Y83lEt+xLbS203bTcnJiZ62B2AXvQS9lZvAnzl2tuIGI+IRkQ0xsbGetgdgF70EvYdkuZPeTxP0oe9tQOgX3oJ++uSTrd9qu2jJf1Q0nP1tAWgbl0PvUXEAdu3Svo3TQ69rYqId2vrDECtehpnj4gXJb1YUy8A+ojLZYEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiip1lcMRgbNmwo1q+99tq2tU2bNhW3Pfnkk7vqCV8/PYXd9jZJ+yQdlHQgIhp1NAWgfnWc2f82IvbU8HsA9BGv2YEkeg17SPq97TdsL221gu2ltpu2mxMTEz3uDkC3eg37ooj4jqRLJd1i+7uHrhAR4xHRiIjG2NhYj7sD0K2ewh4RH1a3uyWtkXROHU0BqF/XYbd9rO1vfnFf0vclba6rMQD16uXd+JMkrbH9xe/5l4j411q6wpesWbOmWK/+H+AQn376advazJkzB9jJaOg67BHxvqSza+wFQB8x9AYkQdiBJAg7kARhB5Ig7EASfMR1BOzYsaNYf+yxx7r+3QcPHux621H31FNPFetvvfVW29r9999fdzsjjzM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOPsI+OCDD4r1jz/+uFhftGhR29rcuXO76mkU7Nu3r1hftmxZsX7vvffW2c7XHmd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYR0Otnq2+//faaOhktDz30UE/bL168uKZOjgyc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZB6D0/eWS9NJLLxXrEVGsL1my5LB7GgV79uwp1pcvX16sX3jhhcX6aaeddtg9Hck6ntltr7K92/bmKcuOt73O9tbq9rj+tgmgV9N5Gv8bSZccsmyFpPURcbqk9dVjACOsY9gj4lVJew9ZfKWk1dX91ZKuqrkvADXr9g26kyJipyRVtye2W9H2UttN282JiYkudwegV31/Nz4ixiOiERGNsbGxfu8OQBvdhn2X7TmSVN3urq8lAP3Qbdifk3RDdf8GSWvraQdAv3QcZ7f9pKTvSZpte4ekn0m6T9LvbN8sabuka/vZ5Nfd9u3bi/X9+/cX67brbGegPvvss7a1u+++u7htp//uVatWddVTVh3DHhHXtSnxzQDA1wiXywJJEHYgCcIOJEHYgSQIO5AEH3FFX23cuLFt7cEHHyxue+655xbr8+bN66qnrDizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMPwKxZs4r1GTPK/xsOHDhQrL/88sttaxdddFFx234rjaV3Oi4PP/xwsX700Ud31VNWnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Qdg8eLyF/GuXLmyWL/rrruK9WuuuaZt7emnny5u2+s4/AsvvFCsP/HEE21rF1xwQXHbs846q6ue0BpndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2EbBs2bJivdlsFutr1qxpW7viiiuK265YsaJY73SNwGuvvVasl1x99dVdb9urrVu3Fuvr168v1s8777xifeHChYfdU791PLPbXmV7t+3NU5attP2B7U3Vz2X9bRNAr6bzNP43ki5psfyXEbGw+nmx3rYA1K1j2CPiVUl7B9ALgD7q5Q26W22/XT3NP67dSraX2m7abk5MTPSwOwC96Dbsv5b0LUkLJe2U9PN2K0bEeEQ0IqIxNjbW5e4A9KqrsEfErog4GBGfS3pY0jn1tgWgbl2F3facKQ+vlrS53boARoMjoryC/aSk70maLWmXpJ9VjxdKCknbJP04InZ22lmj0YhOY8Y4fDfddFPb2uOPP17cttN30veq9Pd1xhlnFLe98cYba+7m/z3zzDPF+jHHHFOsP//888V6p+/E75dGo6Fms+lWtY4X1UTEdS0WP9JzVwAGistlgSQIO5AEYQeSIOxAEoQdSKLj0FudGHobvFdeeaVYX7t2bbE+Pj5erO/fv79YL/192S1HiAai00dQH3jggWL9/PPPr7Od2pSG3jizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLOj6MQTTyzWTzjhhGK99PHbmTNnFre9/vrri/XSdNCSdPHFF7etnXrqqcVtO/U2qhhnB0DYgSwIO5AEYQeSIOxAEoQdSIKwA0kwZXNyGzZsKNY/+eSTYr3TOPvy5csPu6fpuu222/r2u49EnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2ZPbu3dvsT5jRvlP5J577qmzHfRRxzO77fm2/2B7i+13bf+kWn687XW2t1a3x/W/XQDdms7T+AOSfhoRZ0g6T9Itts+UtELS+og4XdL66jGAEdUx7BGxMyLerO7vk7RF0lxJV0paXa22WtJV/WoSQO8O6w062wskfVvSRkknRcROafIfBEktv6zM9lLbTdvNiYmJ3roF0LVph932LEnPSLo9IsqfjpgiIsYjohERjbGxsW56BFCDaYXd9jc0GfQnIuLZavEu23Oq+hxJu/vTIoA6dBx68+S8uo9I2hIRv5hSek7SDZLuq27Lc/9iJD377LPF+imnnFKsL1mypM520EfTGWdfJOlHkt6xvaladqcmQ/472zdL2i7p2v60CKAOHcMeEX+U1PJL5yUtrrcdAP3C5bJAEoQdSIKwA0kQdiAJwg4kwUdcj3Cdvgp63bp1xfrZZ59dZzsYIs7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+xHuEcffbRY/+ijj4r1O+64o852MESc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZj3D79+/vafvLL7+8pk4wbJzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0R5BXu+pN9KOlnS55LGI+JXtldK+jtJE9Wqd0bEi6Xf1Wg0otls9tw0gNYajYaazWbLWZenc1HNAUk/jYg3bX9T0hu2v5hZ4JcR8U91NQqgf6YzP/tOSTur+/tsb5E0t9+NAajXYb1mt71A0rclbawW3Wr7bdurbB/XZpultpu2mxMTE61WATAA0w677VmSnpF0e0R8IunXkr4laaEmz/w/b7VdRIxHRCMiGmNjYzW0DKAb0wq77W9oMuhPRMSzkhQRuyLiYER8LulhSef0r00AveoYdtuW9IikLRHxiynL50xZ7WpJm+tvD0BdpvNu/CJJP5L0ju1N1bI7JV1ne6GkkLRN0o/70iGAWkzn3fg/Smo1blccUwcwWriCDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kETHr5KudWf2hKT/mbJotqQ9A2vg8Ixqb6Pal0Rv3aqzt7+OiJbf/zbQsH9l53YzIhpDa6BgVHsb1b4keuvWoHrjaTyQBGEHkhh22MeHvP+SUe1tVPuS6K1bA+ltqK/ZAQzOsM/sAAaEsANJDCXsti+x/Z+237O9Yhg9tGN7m+13bG+yPdT5pas59Hbb3jxl2fG219neWt22nGNvSL2ttP1Bdew22b5sSL3Nt/0H21tsv2v7J9XyoR67Ql8DOW4Df81u+yhJ/yXpYkk7JL0u6bqI+I+BNtKG7W2SGhEx9AswbH9X0p8l/TYizqqW/aOkvRFxX/UP5XER8fcj0ttKSX8e9jTe1WxFc6ZOMy7pKkk3aojHrtDXDzSA4zaMM/s5kt6LiPcj4i+SnpJ05RD6GHkR8aqkvYcsvlLS6ur+ak3+sQxcm95GQkTsjIg3q/v7JH0xzfhQj12hr4EYRtjnSvrTlMc7NFrzvYek39t+w/bSYTfTwkkRsVOa/OORdOKQ+zlUx2m8B+mQacZH5th1M/15r4YR9lZTSY3S+N+iiPiOpEsl3VI9XcX0TGsa70FpMc34SOh2+vNeDSPsOyTNn/J4nqQPh9BHSxHxYXW7W9Iajd5U1Lu+mEG3ut095H7+zyhN491qmnGNwLEb5vTnwwj765JOt32q7aMl/VDSc0Po4ytsH1u9cSLbx0r6vkZvKurnJN1Q3b9B0toh9vIlozKNd7tpxjXkYzf06c8jYuA/ki7T5Dvy/y3pH4bRQ5u+/kbSv1c/7w67N0lPavJp3WeafEZ0s6QTJK2XtLW6PX6EentM0juS3tZksOYMqbcLNPnS8G1Jm6qfy4Z97Ap9DeS4cbkskARX0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8L0uMqEcKFEicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[142], interpolation='nearest', cmap=plt.cm.binary)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[4354], interpolation='nearest', cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes=10)\n",
    "Y_test = to_categorical(Y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split the data into Trainingdata and Testdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 234\n",
    "X_train, X_train_val, Y_train, Y_train_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Expand the dimensions of the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the \n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_train_val = np.expand_dims(X_train_val, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build the model of the neuronal network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Choose an Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer , loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configure the Callback \"ReduceLROnPlateau\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.7, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Configure the Callback \"ModelCheckpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now() \n",
    "folder_name = f\"./model/{now.strftime('%H-%M-%S')}\" \n",
    "os.mkdir(folder_name)\n",
    "\n",
    "filepath_check=folder_name+\"/weights-improvement-{epoch:02d}-{val_acc:.6f}.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath_check,\n",
    "    monitor=\"val_acc\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    ")\n",
    "filepath_best=folder_name+\"/weights.best.h5\"\n",
    "checkpoint_best = ModelCheckpoint(filepath_best, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Setup the ImageDataGenerator for Data Augementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.07, # Randomly zoom image \n",
    "        width_shift_range=0.12,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.12,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "training = datagen.flow(X_train,Y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.5201 - acc: 0.8262 - val_loss: 0.0755 - val_acc: 0.9783\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1181 - acc: 0.9657 - val_loss: 0.0520 - val_acc: 0.9833\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0884 - acc: 0.9744 - val_loss: 0.0437 - val_acc: 0.9863\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0714 - acc: 0.9798 - val_loss: 0.0378 - val_acc: 0.9888\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0620 - acc: 0.9823 - val_loss: 0.0465 - val_acc: 0.9882\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0601 - acc: 0.9829 - val_loss: 0.0353 - val_acc: 0.9893\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0554 - acc: 0.9847 - val_loss: 0.0363 - val_acc: 0.9907\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0484 - acc: 0.9862 - val_loss: 0.0302 - val_acc: 0.9917\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0487 - acc: 0.9854 - val_loss: 0.0422 - val_acc: 0.9888\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0456 - acc: 0.9875 - val_loss: 0.0286 - val_acc: 0.9930\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0451 - acc: 0.9869 - val_loss: 0.0332 - val_acc: 0.9900\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0435 - acc: 0.9872- ETA: 0s - loss: 0.0449 -\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0435 - acc: 0.9872 - val_loss: 0.0288 - val_acc: 0.9915\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0323 - acc: 0.9902 - val_loss: 0.0230 - val_acc: 0.9938\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0305 - acc: 0.9912 - val_loss: 0.0210 - val_acc: 0.9942\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0305 - acc: 0.9914 - val_loss: 0.0319 - val_acc: 0.9903\n",
      "Epoch 16/30\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9915\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0299 - acc: 0.9916 - val_loss: 0.0282 - val_acc: 0.9925\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0230 - acc: 0.9930 - val_loss: 0.0245 - val_acc: 0.9925\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0222 - acc: 0.9931\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0222 - acc: 0.9931 - val_loss: 0.0296 - val_acc: 0.9925\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0234 - val_acc: 0.9940\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0174 - acc: 0.9945 - val_loss: 0.0217 - val_acc: 0.9943\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0210 - val_acc: 0.9940\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0182 - acc: 0.9946- ETA: 0s - loss: 0.0\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0182 - acc: 0.9946 - val_loss: 0.0229 - val_acc: 0.9942\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0233 - val_acc: 0.9942\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0196 - val_acc: 0.9945\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0168 - acc: 0.9951 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.0210 - val_acc: 0.9950s - loss: 0.0129 - acc: \n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0224 - val_acc: 0.9940\n",
      "Epoch 29/30\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9969\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.0215 - val_acc: 0.9940\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0125 - acc: 0.9964 - val_loss: 0.0202 - val_acc: 0.9955\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training, epochs = epochs, validation_data = (X_train_val, Y_train_val),\n",
    "                              verbose = 1, steps_per_epoch=300\n",
    "                              , callbacks=[learning_rate_reduction, checkpoint_best])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test the model on unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classified handwritten digits (nominal) => 5973\n",
      "Correct classified handwritten digits (relative) => 99.55%\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(X_train_val)\n",
    "count = 0\n",
    "for i, result in enumerate(results):\n",
    "    if np.argmax(result) == np.argmax(Y_train_val[i]): count += 1\n",
    "    \n",
    "print(f\"Correct classified handwritten digits (nominal) => {count}\")\n",
    "print(f\"Correct classified handwritten digits (relative) => {round((count/Y_train_val.shape[0])*100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
